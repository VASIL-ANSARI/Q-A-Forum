<p>You are hitting fundamental laws of complexity here. You are trying to process massive numbers of documents for massive numbers of words and produce massive data sets from that.</p>

<p>It's always going to be slow.</p>

<p>Some things that may speed it up:</p>

<ol>
<li><p>Forget the list of a million words. Instead just accept any word as you find it in the text, you can always filter them later. If you do need to filter against the list then make sure the list is in an appropriate form (for example a HashSet) which allows you to check quickly.</p></li>
<li><p>This sort of thing is more likely to be IO bound than CPU bound so try running it on fast SSD drives - or if the files are small enough set up a RAM disk and run it from that. Do some monitoring to identify just where the bottlenecks are.</p></li>
<li><p>The processing on each set of files is as you've already identified very parallel so you could look at spreading it out not just over multiple cores but multiple machines.</p></li>
</ol>

<p>Something to try (the overhead of database may actually make it slower):
Rather than doing the merge at the end you could just compile the results for processing one document together in memory. Once you finish processing then do a single batch insert into a database. The database will then allow you to query the results dynamically, using sum() etc to find the totals for each word combination. This actually gives you a more flexible/useful result than just the flat file and avoids the separate merge step.</p>

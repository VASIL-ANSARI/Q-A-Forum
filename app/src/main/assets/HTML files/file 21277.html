<blockquote>
  <p>1) By what I've heard, it seems like Google uses python for his
  web-crawlers, is that right?</p>
</blockquote>

<p>An early version of Google used Python for the webcrawler.  This is indicated in early publications dating back to the 90s (see <a href="http://infolab.stanford.edu/~backrub/google.html">Anatomy of a Search Engine</a>.)  Only a Google employee can tell you whether they still use Python today for their crawler.</p>

<blockquote>
  <p>2) Following that question, would you say that it is a good choice? Is
  Python the most suited language for that kind of things? Why?</p>
</blockquote>

<p>There is no way to answer this objectively.  It's almost equivalent to asking "is Python a good language?"  Pros of using Python for a webcrawler include the various web and networking libraries, parsers, etc. which are readily available in Python, as well as the fact that Python's performance drawbacks aren't likely to matter much for most webcrawlers, since a webcrawler typically spends all of it's time waiting for URLs to resolve and download.</p>

<blockquote>
  <p>3) What's legal to do with web-crawlers, and what is not? I've heard
  that many website won't really appreciate that you download too many
  of their pages, but isn't that exactly what Google do? It seems like a
  big grey area and I'd like to know how I can make sure what I do is
  legal...</p>
</blockquote>

<p>It's not a question of legality so much as it is a question of politeness.  If you bombard a webserver with too many requests at once, it's likely to ban your IP.  Commercial webcrawlers such as GoogleBot generally use crawl-wait times, so that there is a delay between requests to the same host.  A well-behaved webcrawler also must obey the <a href="http://en.wikipedia.org/wiki/Robots_exclusion_protocol">Robots Exclusion Protocol</a>.</p>

<blockquote>
  <p>4) If you have any good tutorial on how to make web-crawlers (the
  programming language isn't important), I would really appreciate a
  link to it!</p>
</blockquote>

<p>Conceptually, a webcrawler is just a FIFO queue of URLs.  You choose a starting URL, place it at the top of the queue, pop and download the URL, parse the HTML and extract the links, and then add each link to the queue.  Then pop the top URL and repeat ad infinitum or until some stopping condition occurs.  </p>

<p>However, in practice, a robust webcrawler is much more complicated to write because you need to account for many, many issues, such as avoiding downloading duplicate URLs, implementing crawl-wait times for each host, obeying the Robots Exclusion Protocol, avoiding <a href="http://en.wikipedia.org/wiki/Spider_trap">crawler traps</a>, etc.  You also probably need to include logic for some kind of decent crawl strategy so your crawler doesn't spend most of it's time downloading useless garbage, as the signal-to-noise ratio of the modern World Wide Web is pretty abysmal.</p>

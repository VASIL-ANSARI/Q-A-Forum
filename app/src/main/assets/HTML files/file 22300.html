<p>In general you can not load whole file into memory of Java VM.
You should find some streaming solution to process large files - read data chunk by chunk and save the results w/o fixing in memory whole data set <br>
This specific task - unzip is probably not suited for the MR since there is no logical division of data into records. <br>
Please also note that hadoop is handling gzip automatically - your input stream will be already decompressed. <br></p>

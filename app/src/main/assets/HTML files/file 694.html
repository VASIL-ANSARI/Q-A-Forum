<p><code>unicode</code> does not guess the encoding of your text. If your object can print itself as <code>unicode</code>, define the <code>__unicode__()</code> method that returns a Unicode string.</p>

<p><hr /></p>

<p>The secret is that <code>unicode(r)</code> is not actually calling <code>__str__()</code> itself. Instead, it's looking for a <code>__unicode__()</code> method. The default implementation of <code>__unicode__()</code> will call <code>__str__()</code> and then attempt to decode it using the <code>ascii</code> charset. When you pass the encoding, <code>unicode()</code> expects the first object to be something that can be decoded -- that is, an instance of <code>basestring</code>.</p>

<p><hr /></p>

<blockquote>
  <p>Behavior is weird because it tries to decode as ascii if I don't pass 'utf-8'. But if I pass 'utf-8' it gives a different error...</p>
</blockquote>

<p>That's because when you specify "utf-8", it treats the first parameter as a string-like object to be decoded. Without it, it treats the parameter as an object to be coerced to unicode.</p>

<p>I do not understand the confusion. If you know that the object's <code>text</code> attribute will always be UTF-8 encoded, just define <code>__unicode__()</code> and then everything will work fine.</p>

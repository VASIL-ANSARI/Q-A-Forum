<p>This is a terrible (poorly conditioned) matrix to run principal components analysis on. It has an eigenvalue of zero (which by itself may be problematic), and the remaining eigenvalues of 1 (you can subtract rows from one another to get a degenerate matrix). Python may have a poor implementation of eigensystem solver that relies on the matrix being reasonably regular (all eigenvalues are distinct and sufficiently well separated from zero and from each other). I am not familiar with the method, but my feeling, based on the title of Fast Fixed Point, is that they rely on the properties of multiplication of blowing up the eigenvalues: if <i>A</i>=&Sigma;<sub><i>k</i></sub> <i>&lambda;</i><sub><i>k</i></sub> <i>u</i><sub><i>k</i></sub> <i>u</i><sub><i>k</i></sub>' for the appropriate orthogonal vectors <i>u</i><sub><i>k</i></sub>, and <i>&lambda;</i><sub>1</sub> > <i>&lambda;</i><sub>2</sub> > &#x2026; > <i>&lambda;</i><sub><i>p</i></sub> > 0, then <i>A</i><sup><i>n</i></sup> &#x2248; <i>&lambda;</i><sub>1</sub> <i>u</i><sub>1</sub> <i>u</i><sub>1</sub>' for sufficient large power <i>n</i>. This idea simply does not work when you feed an array of ones as inputs: you just keep getting the top eigenvalue of one that does not separate from others. Worse, for the specific matrix that you feed in, (7<i>&#xb7;</i>10<sup>-5</sup>)<sup>20</sup> gets close to what can be represented as a double precision number. In the end, you may have a complete crap in the output. There are appropriate computational linear algebra methods that are much more computationally stable and reliable. A decision to implement one or the other method is the developer's judgment call; besides the <em>fast</em> part of it, one would also need to think about how robust the method is. No offense, but I would take a stable slow method over a quick and dirty method on most occasions.</p>

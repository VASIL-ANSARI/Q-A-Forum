<p>You can use the "settings" notion and the <code>-s</code> command-line argument:</p>

<pre><code>scrapy crawl internallinkspider -s FILE_NAME="stuff"
</code></pre>

<p>Then, in the pipeline:</p>

<pre><code>import csv

class CsvWriterPipeline(object):
    @classmethod
    def from_crawler(cls, crawler):
        settings = crawler.settings
        file_name = settings.get("FILE_NAME")
        return cls(file_name)

    def __init__(self, file_name):
        header = ["URL"]
        self.csvwriter = csv.writer(open(file_name, 'wb'))
        self.csvwriter.writerow(header)

    def process_item(self, item, internallinkspider):
        # build your row to export, then export the row
        row = [item['url']]
        self.csvwriter.writerow(row)
        return item
</code></pre>

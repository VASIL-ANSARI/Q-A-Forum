<p>It would appear that <code>numpy.bool_</code> behaves slightly differently to vanilla Python <code>bool</code>:</p>

<pre><code>&gt;&gt;&gt; int(True+True) == int(True) + int(True)
True
&gt;&gt;&gt; int(numpy.bool_(1)+numpy.bool_(1)) == int(numpy.bool_(1)) + int(numpy.bool_(1))
False
</code></pre>

<p>This is because:</p>

<pre><code>&gt;&gt;&gt; True+True
2
&gt;&gt;&gt; numpy.bool_(1)+numpy.bool_(1)
True
&gt;&gt;&gt; int(numpy.bool_(1)+numpy.bool_(1))
1
</code></pre>

<p>Basically, the addition operation for <code>numpy.bool_</code> is logical, rather than numerical; to get the same behaviour with <code>bool</code>:</p>

<pre><code>&gt;&gt;&gt; int(True and True)
1
</code></pre>

<p>This is fine if you only use it for truthiness, as intended, but if you try to use it in an integer context without being explicit about that, you end up surprised. As soon as you're explicit, expected behaviour is restored:</p>

<pre><code>&gt;&gt;&gt; int(numpy.bool_(1)) + int(numpy.bool_(1))
2
</code></pre>

<p>Why not compute the statistics on a subset of the original data? For example, here we compute the mean and std for just 100 points:</p>

<pre><code>sample_size = 100
data_train = np.random.rand(1000, 20, 10, 10)

# Take subset of training data
idxs = np.random.choice(data_train.shape[0], sample_size)
data_train_subset = data_train[idxs]

# Compute stats
mean = np.mean(data_train_subset, axis=(0,2,3))
std = np.std(data_train_subset, axis=(0,2,3))
</code></pre>

<p>If your data is 1.7Gb, it is highly unlikely that you need all the data to get an accurate estimation of the mean and std.</p>

<p>In addition, could you get away with fewer bits in your datatype? I'm not sure what datatype <code>caffe.io.datum_to_array</code> returns, but you could do:</p>

<pre><code>data = caffe.io.datum_to_array(datum).astype(np.float32)
</code></pre>

<p>to ensure the data is <code>float32</code> format. (If the data is currently <code>float64</code>, then this will save you half the space).</p>

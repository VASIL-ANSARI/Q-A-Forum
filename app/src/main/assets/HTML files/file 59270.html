<p>You need to do something like this:</p>

<pre><code>@app.task
def pan_task():
    url = 'http://feeds.example.com/reuters/technologyNews'
    name = 'noticiassin'
    live_leaks = [i for i in feedparser.parse(url).entries][:10]
    the_count = len(live_leaks)
    ky = feedparser.parse(url).keys()
    oky = [i.keys() for i in feedparser.parse(url).entries][1] # shows what I can pull

    def make_soup(url):
        def swappo():
            user_one = ' "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0" '
            user_two = ' "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_5)" '
            user_thr = ' "Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko" '
            user_for = ' "Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:10.0) Gecko/20100101 Firefox/10.0" '

            agent_list = [user_one, user_two, user_thr, user_for]
            a = random.choice(agent_list)
            return a
        headers = {
            "user-agent": swappo(),
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "accept-charset": "ISO-8859-1,utf-8;q=0.7,*;q=0.3",
            "accept-encoding": "gzip,deflate,sdch",
            "accept-language": "en-US,en;q=0.8",
        }
        the_comments_page = requests.get(url, headers=headers)
        soupdata = BeautifulSoup(the_comments_page.text, 'html5lib')
        # comment = soupdata.find('div')
        # para = comment.find_all('p')
        # kids = [child.text for child in para]
        # blu = str(kids).strip('[]')
        return soupdata

    live_entries = []
    try:
        for live_leak in live_leaks:
            live_entries.append(make_soup(live_leak.links[0]['href']))
            # Do what ever you need to do to o here
    except IndexError:
        print('error check logs')
        live_entries = []
    return live_entries
</code></pre>

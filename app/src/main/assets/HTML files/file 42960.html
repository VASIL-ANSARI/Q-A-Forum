<p>The typical pattern when using semaphores to control access to a limited resource is </p>

<ul>
<li><p>Create semaphore with non-zero value;</p></li>
<li><p>For every task:</p>

<ul>
<li><p>When starting, "wait" for semaphore (thereby consuming one of the available signals, or if one not available, wait for one); and</p></li>
<li><p>When done, "signal" semaphore (making it available for another task).</p></li>
</ul></li>
</ul>

<p>So, let's say you wanted to start 1,000,000 tasks, only 4 concurrently at any given time, you could do something like:</p>

<pre><code>dispatch_semaphore_t semaphore = dispatch_semaphore_create(4);

dispatch_queue_t queue = ... // some concurrent queue, either global or your own

dispatch_async(queue, ^{
    for (long index = 0; index &lt; 1000000; index++) {
        dispatch_semaphore_wait(semaphore, DISPATCH_TIME_FOREVER);
        dispatch_async(queue, ^{
            [self performSomeActionWithIndex:index completion:^{
                dispatch_semaphore_signal(semaphore);
            }];
        });
    }
});
</code></pre>

<p>Clearly, if you're dynamically adding more tasks to perform, you could change this from a <code>for</code> loop into a <code>while</code> loop, checking some synchronized source, but the idea is the same. </p>

<p>The key observation, though, is that I don't have <code>performSomeActionWithIndex</code>, itself, recursively creating tasks itself (because then you get into the deadlock situation of the original question where tasks are stopped because they can't start new tasks). </p>

<p>Now, I don't know if your problem can be refactored into this sort of pattern, but if you can, this might be an option.</p>

<hr>

<p>By the way, for the sake of completeness, I'd point out that the typical solution for controlling the degree of concurrency is to use operation queues rather than dispatch queues, in which case you can specify <code>maxConcurrentOperationCount</code>. </p>

<p>As you correctly pointed out, there are memory implications of that. In my tests, each scheduled operation takes up at least 500 bytes (and likely more in real-world scenarios), so if you really have more than, say, 5,000-10,000 tasks to be scheduled, operation queues may quickly become impractical. As you advised, future readers should refer to the <a href="https://developer.apple.com/library/mac/documentation/General/Conceptual/ConcurrencyProgrammingGuide/ConcurrencyandApplicationDesign/ConcurrencyandApplicationDesign.html#//apple_ref/doc/uid/TP40008091-CH100-SW6" rel="nofollow">Performance Implications</a> section in <em>Concurrency Programming Guide: Concurrency and Application Design.</em></p>

<p>I know that this is not a viable approach in your case, but I only mention it for the benefit of future readers. I would generally advise the use of operation queues when one needs to control the degree of concurrency. I would only jump to an approach like the one outlined above if you're dealing with so many tasks that one can't reasonably just schedule them on an operation queue.</p>

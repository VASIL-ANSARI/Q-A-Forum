<p>For your second question: <em>One additional question is how to run a spark program using a java code (from another program)?</em> 
In your java program if you have a Maven project, you can add a maven dependency.
Spark artifacts are hosted in Maven Central. You can add a Maven dependency in your pom.xml</p>

<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
   &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
   &lt;version&gt;1.3.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>If not, you can choose to download the jar file and add it as an external library to your java code. 
After any of this options, you will be able to write java spark code in your java program.
You can find the dependency and the jar file here: <a href="http://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10/1.4.1" rel="nofollow">http://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10/1.4.1</a>
Yes, you need a machine with Spark to run your code. In order to run it you can make an executable jar out of your program and run it with:</p>

<pre><code>$: spark/bin/spark-submit --class com.your.MainClass --master spark://your_master_node:7077 your_jar_file.jar
</code></pre>

<p>Good luck!</p>

<p>It is not a problem.</p>

<p>First, note that 0 â¤ a &lt; 1, so errors in the average tend to diminish, not accumulate. Incoming new data displaces old errors.</p>

<p>Subtracting floating-point numbers of similar magnitude (and same sign) does not lose absolute accuracy. (You wrote âprecisionâ, but precision is the fineness with which values are represented, e.g., the width of the <code>double</code>type, and that does not change with subtraction.) Subtracting numbers of similar magnitude may cause an increase of relative error: Since the result is smaller, the error is larger relative to it. However, the relative error of an intermediate value is of no concern.</p>

<p>In fact, subtracting two numbers, each of which equals or exceeds half the other, has no error: The correct mathematical result is exactly representable (Sterbenzâ Lemma).</p>

<p>So the subtraction in the latter operation sequence is likely to be exact or low-error, depending on how much the values fluctuate. Then the multiplication and the addition have the usual rounding errors, and they are not particularly worrisome unless there are both positive and negative values, which can lead to large relative errors when the average is near zero. If a fused multiply-add operation is available (See <code>fma</code> in <code>&lt;tgmath.h&gt;</code>), then you can eliminate the error from the multiplication.</p>

<p>In the former operation sequence, the evaluation of <code>1-a</code> will be exact if <code>a</code> is at least Â½. That leaves two multiplications and one addition. This will tend to have very slightly greater error than the latter sequence, but likely not enough to notice. As before, old errors will tend to diminish.</p>

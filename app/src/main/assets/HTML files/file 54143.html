<p>The <code>String</code> value <code>Ã©Ã Ã¨</code> is encoded in UTF-8 as byte octets <code>0xC3 0xA9 0xC3 0xA0 0xC3 0xA8</code>.  Those same byte octets interpreted as Windows-1252 are the String value <code>ÃÂ©Ã&lt;nbsp&gt;ÃÂ¨</code> (where <code>&lt;nbsp&gt;</code> is a <em>non-breaking space</em> character, Unicode codepoint <code>U+00A0</code>).</p>

<p>In the first example, you are converting a <code>String</code> to the above UTF-8 bytes, and then you are converting the bytes back to a <code>String</code> using Windows-1252 instead of UTF-8.  So you should be getting a new <code>String</code> value of <code>ÃÂ©Ã&lt;nbsp&gt;ÃÂ¨</code>, not <code>Ã©Ã Ã¨</code>. You are then writing that <code>String</code> to the console, so it gets encoded using Windows-1252 back to byte octets <code>0xC3 0xA9 0xC3 0xA0 0xC3 0xA8</code>, which should be displayed as <code>ÃÂ©Ã&lt;nbsp&gt;ÃÂ¨</code> (or something similar to it) if the console is displaying the bytes as-is.  On the other hand, if the console is configured for UTF-8 instead, those bytes would display as <code>Ã©Ã Ã¨</code> when interpreted as UTF-8.</p>

<p>In the second example, since you are using Windows-1252 for both encoding and decoding, and the particular characters in question are supported by Windows-1252, you should end up with the original <code>String</code> value <code>Ã©Ã Ã¨</code> before writing it to the console.  If that <code>String</code> gets encoded to bytes using Windows-1252, and the console is configured for UTF-8, it would make sense why you don't see <code>Ã©Ã Ã¨</code> displayed.  The <code>String</code> value <code>Ã©Ã Ã¨</code> is encoded in Windows-1252 as byte octets <code>0xE9 0xE0 0xE8</code>, which is not a valid UTF-8 byte octet sequence.</p>

<p>In short, the behavior you are seeing would happen when your console is configured to interpret outgoing bytes as UTF-8, but you are not giving it proper UTF-8 encoded bytes as output.</p>

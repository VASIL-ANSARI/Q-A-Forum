<p>UNIX timestamps count in seconds, JavaScript timestamps count in milliseconds.</p>

<p>You should just multiply the passed timestamp by 1000, e.g.:</p>

<pre><code>var date = new Date(timestamp * 1000);
</code></pre>

<p>This will not only fix the initial conversion, but ensure that when you add a second (in the timer callback) that you actually do add 1 second, and not just 1 millisecond.  The latter is the reason that you <em>appear</em> to be getting the same <code>Date</code> object back - you're almost certainly not, but the new one is only 1ms later than the previous so will show the same <code>HH:MM:SS</code> value <em>most</em> of the time.</p>

<p>In practise, note that you'll find that <code>setTimeout</code> does not guarantee that the events will fire 1000ms apart so you will get some clock drift.</p>

<p>You ought to take into account how long the preceding code takes to run too - indeed a better approach may be to simply determine the difference between the originally supplied timestamp and the local computer's time, and use that as a reference value for all subsequent calls.</p>

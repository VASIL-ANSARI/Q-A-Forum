<p>You can't ignore errors<sup>1</sup>.  You can only ignore warnings.  Change the code.</p>

<pre><code>printf("\\x%02x", ((unsigned char *)code)[i]);
</code></pre>

<p><strong>It's just a guess,</strong> since without documentation or input from the original author of the code, we have no solid evidence for what the code is actually supposed to do.  However, the above correction is <em>extremely plausible,</em> it's a simple typo (the original author forgot a <code>\</code>), and it's conceivable that the author uses a C compiler which silently ignores the error (Python has the same behavior by design).</p>

<p>The line of code above, or something almost exactly like it, is found in probably tens of thousands of source files across the globe.  It is used for encoding a binary blob using escape sequences so it can be embedded as a literal in a C program.  Similar code appears in JSON, XML, and HTML emitters.  I've probably written it a hundred times.</p>

<p><strong>Alternatively,</strong> if the code were supposed to print out the character, this would not work:</p>

<pre><code>printf("\x%02x", ((unsigned char *)code)[i]);
</code></pre>

<p>This doesn't work because escape sequences (the things that start with <code>\</code>, like <code>\x42</code>) are handled by the C compiler, but format strings (the things that start with <code>%</code>, like <code>%02x</code>) are handled by <code>printf</code>.  The above line of code might only work if the order were reversed: if <code>printf</code> ran first, <em>before</em> you compiled the program.  So no, it doesn't work.</p>

<p>If the author had intended to write literal characters, the following is more plausible:</p>

<pre><code>printf("%c", ((unsigned char *)code)[i]);  // clumsy
putchar((unsigned char *)code)[i]);        // simpler
</code></pre>

<p>So you know either the original author simply typo'd and forgot a single <code>\</code> (I make that mistake <em>all the time</em>), or the author has no clue.</p>

<p><strong>Notes:</strong></p>

<p><sup>1</sup>: An error means that GCC doesn't <em>know</em> what the code is supposed to do, so continuing would be impossible.</p>

<p>The problem lies in your <code>Gradient</code> function. In <code>numpy</code> assignment is <strong>not copying objects</strong>, so your line</p>

<pre><code>reg = theta
</code></pre>

<p>makes <code>reg</code> a reference to <code>theta</code>, so each time you compute gradient you actually modify your current solution. It should be</p>

<pre><code>reg = theta.copy()
</code></pre>

<p>I would also suggest starting from random weights</p>

<pre><code>initial_theta = np.random.randn(n+1)
</code></pre>

<p>Now solution is no longer zeros (although I did not check each formula, so there still might be mathematical error). It is also worth noting, that for linearly separable problems, logistic regression without regularization is ill-posed (its objective is unbounded), so I suggest testing with <code>lam&gt;0</code>.</p>

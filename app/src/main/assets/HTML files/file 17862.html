<p>The size and meaning of <code>wchar_t</code> is implementation-defined. On Windows it's 16 bit as you say, on Unix-like systems it's often 32 bit but not always.</p>

<p>For that matter, a compiler is permitted do its own thing and pick a different size for <code>wchar_t</code> than what the system says -- it just won't be ABI-compatible with the rest of the system.</p>

<p>C++11 provides <code>std::u32string</code>, which is for representing strings of unicode code points. I believe that sufficiently recent Microsoft compilers include it. It's of somewhat limited use since Microsoft's system functions expect 16-bit wide characters (a.k.a UTF-16le), not 32-bit unicode code points (a.k.a UTF-32, UCS-4).</p>

<p>You mention UTF-8, though: UTF-8 encoded data can be stored in a regular <code>std::string</code>. Of course since it's a variable-length encoding, you can't access unicode code points by index, you can only access the bytes by index. But you'd normally write your code not to need to access code points by index anyway, even if using <code>u32string</code>. Unicode code points don't correspond 1-1 with printable characters ("graphemes") because of the existence of combining marks in Unicode, so many of the little tricks you play with strings when learning to program (reversing them, searching for substrings) don't work so easily with Unicode data no matter what you store it in.</p>

<p>The character í¡í½¢ is, as you say, \u24B62. It is UTF-8 encoded as a series of <em>four</em> bytes, not three: F0 A4 AD A2. Translating between UTF-8 encoded data and unicode code points is effort (admittedly not a huge amount of effort and library functions will do it for you). It's best to regard "encoded data" and "unicode data" as separate things. You can use whatever representation you find most convenient right up to the point where you need to (for example) render the text to screen. At that point you need to (re-)encode it to an encoding that your output destination understands.</p>

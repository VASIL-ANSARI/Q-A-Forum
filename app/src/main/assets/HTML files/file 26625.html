<p>As noted in the comment by @larsmans this can be solved by <a href="http://en.wikipedia.org/wiki/Reinforcement_learning" rel="nofollow">Reinforcement Learning</a> paradigm. In the context of neural networks currently the most popular (and only?) approach is to use two neural networks: </p>

<ul>
<li><p>actor network: which learns what action (propeller power in this case) the <em>agent</em> is ought to take in a given <em>state</em> (vertical speed in this case)</p></li>
<li><p>critic network: which learns values, in the terms of future reinforcement <em>agent</em> can "hope" to achieve from this <em>state</em></p></li>
</ul>

<p>This approach is known as <a href="http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html" rel="nofollow">Actor-Critic</a> methods. All you need to do additionally is to design the <em>reinforcement function</em>. In your case it seems quite simple, as it could be equal to the vertical velocity with additional penalty for deviating from some predefined height (otherwise the networks will learn just to wait a while till the propeller falls and stops for itself).</p>

<p>The main issue will be tuning all parameters for all of this to work correctly, however the problem seems very simple so it maybe not be very hard.</p>

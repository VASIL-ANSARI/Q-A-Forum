<p>For the filter the possible measures are little:</p>

<ul>
<li>There exist different compression levels, the more compression the slower. The default or GZIPOutputStream should be fast enough.</li>
<li>GZIPOutputStream has constructors with <code>size</code> to set.</li>
<li>Then there is buffered streaming, and not doing byte-wise <code>int read()</code>.</li>
<li>Code review for plausibility: the original Content-Length header must be removed.</li>
</ul>

<p>For the static content:</p>

<ul>
<li>.bmp are a waste of space</li>
<li>.pdf can be optimized when images repeat, w.r.t. fonts.</li>
<li>.docx is a zip format, so inner image files might be optimized too</li>
</ul>

<p>For dynamic content generation:</p>

<ul>
<li>Fixed documents can be stored (xxxxxx.yyy.gz) with timestamp and then the generation time forfalls. Only of interest after measuring the real bottle neck; likely the network.</li>
<li>The code for delivery should be fast. In general chain streams, try not to write to a ByteArrayOutputStream, but immediately to a BufferedOutputStream(original output stream). Check that the buffering is not done twice. Some wrapping streams check that the wrapped stream is an instanceof a buffered.</li>
</ul>

<p>Production environment:</p>

<ul>
<li>Maybe you even need throttling (slowing down delivery) in order to serve multiple simultaneous requests.</li>
<li>You may need to do the delivery on another server.</li>
<li>Buy speed from the provider. Inquire from the provider, whether the througput was too high, and the provider slowed things down.</li>
</ul>

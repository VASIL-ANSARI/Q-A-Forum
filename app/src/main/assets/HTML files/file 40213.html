<p>I finally found the problem. For the XOR, I didn't need any bias and it was converging to the expected values. I got exactly the output when you round the final output. What was needed is to train then validate, then train again until the Neural Network is satisfaying. I was training each set until satisfaction but not the WHOLE set again and again.</p>

<pre><code>// Initialize the Neural Network
algorithm.initialize(this.numberOfInputs);

int index = 0;
double errorRate = 0;

// Loop until satisfaction or after some iterations
do {
    // Train the Neural Network
    algorithm.train(this.trainingDataSets, this.numberOfInputs);

    // Validate the Neural Network and return the error rate
    errorRate = algorithm.run(this.validationDataSets, this.numberOfInputs);

    index++;
} while (errorRate &gt; minErrorRate &amp;&amp; index &lt; numberOfTrainValidateIteration);
</code></pre>

<p>With the real datas, I need a bias because the outputs started to diverge. Here is how I added the bias:</p>

<p>In <strong>Neuron.java</strong> class, I added a bias synapse with a weight and an output of 1.0. I sum it with all the other synapses then put it in my activation function.</p>

<pre><code>public class Neuron implements Serializable {

    [...]

    private Synapse bias;

    public Neuron(IActivation activation) {
        [...]
        this.bias = new Synapse(this);
        this.bias.setWeight(0.5); // Set initial weight OR keep the random number already set
    }

    public void updateOutput(double[] inputs) {
        double sumWeights = this.calculateSumWeights(inputs);

        this.output = this.activation.activate(sumWeights + this.bias.getWeight() * 1.0);
    }

    [...]
</code></pre>

<p>In <strong>BackPropagationStrategy.java</strong>, I change the weight and the delta of each bias in the updateWeights method that I renamed updateWeightsAndBias.</p>

<pre><code>public class BackPropagationStrategy implements IStrategy, Serializable {

    [...]

    public void updateWeightsAndBias(NeuralNetwork neuralNetwork, double[] inputs) {

        for (int i = neuralNetwork.getLayers().size() - 1; i &gt;= 0; i--) {

            Layer layer = neuralNetwork.getLayers().get(i);

            for (Neuron neuron : layer.getNeurons()) {

                [...]

                Synapse bias = neuron.getBias();
                double delta = learning * 1.0;
                bias.setWeight(bias.getWeight() + delta + this.momentum * bias.getDelta());

                bias.setDelta(delta);
            }
        }
    }

    [...]
</code></pre>

<p>With the real datas, the Network is converging. It is now a pruning job to find the perfect variables combo (if it is possible) of learning rate, momentum, error rate, quantity of neurons, quantity of hidden layers, etc.</p>
